# ğŸ¥ AskYouTube

**Multimodal Chatbot for YouTube Videos â€” Summarize, Search, and Query with Text and Images**

---

## ğŸš€ Overview

**AskYouTube** is a multimodal chatbot that enables **interactive question-answering and summarization** over YouTube videos.

Given a YouTube URL, the system automatically:

1. **Extracts** the transcript and representative video frames (with timestamps).
2. **Indexes** both modalities (text + images) into a **vector database** for multimodal retrieval.
3. **Generates responses** using a **multimodal language model (LLM)** that reasons over the retrieved video content.

In short â€” you ask questions, and the system understands **whatâ€™s said** _and_ **whatâ€™s shown** in the video to deliver grounded, context-aware answers.

---

## ğŸ” Key Features

- ğŸ¬ **YouTube Video Ingestion** â€” process any video directly from its URL
- ğŸ—£ï¸ **Accurate Transcription** via [OpenAI Whisper](https://github.com/openai/whisper)
- ğŸ–¼ï¸ **Frame Extraction** with timestamp alignment
- ğŸ§© **Multimodal Vector Indexing** â€” store both transcript chunks and visual frames using [OpenAI CLIP](https://github.com/openai/CLIP)
- ğŸ’¬ **Multimodal LLM Inference** â€” accepts text or text + image queries and answers with grounded reasoning, powered by [Ollama](https://ollama.com/)
- ğŸ§¾ **Video Summarization** â€” generate concise summaries of video content
- ğŸ³ **Dockerized Deployment** â€” easy to set up and run anywhere

---

## âœ… Getting Started

<p align="center">
  <img src="assets/askyt_demo.png" alt="Demo" width="500"/>
</p>

### ğŸ§± Prerequisites

- Docker
- NVIDIA Container Toolkit
- GPU with **â‰¥ 12 GB VRAM** (for local LLM hosting)

### ğŸ“¦ Installation

```bash
git clone https://github.com/tien02/askyoutube.git
cp container/.env-example container/.env
```

### â–¶ï¸ Run with Docker

```bash
bash scripts/dev.sh
```

---

## ğŸ’¡ Usage

1. Start the service (via Docker or directly in Python).
2. Provide a YouTube URL through the UI or API.
3. Wait for transcript and frame indexing to complete.
4. Ask a question (text or text + image).
5. Receive an answer generated by the multimodal LLM.

Try the demo with:

```
pip install streamlit
streamlit run app.py
```

---

## ğŸ§  Configuration

You can configure the system through environment variables or a `.env` file:

- `LLM_MODEL` â€” model name or endpoint
- `IMAGE_ENCODER` â€” encoder model name or endpoint
- `VECTOR_DB_URL` â€” connection details for vector storage
- `FRAME_INTERVAL` â€” frame extraction interval (in seconds)

---

## ğŸ§© Project Structure

```
.
â”œâ”€â”€ app/                     # Main entry point (API + service logic)
â”œâ”€â”€ container/               # Environment & deployment files
â”œâ”€â”€ scripts/                 # Ingestion, preprocessing, and helper scripts
â”œâ”€â”€ test/                    # Test suite
â”œâ”€â”€ Dockerfile               # Container definition
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ assets/                  # Example images / demos
```

---

## ğŸ§ª Testing

You can test individual modules using the scripts in the `test/` directory:

```bash
# Test YouTube ingestion and indexing
python test/test_ingest.py <youtube_url>

# Test multimodal chat with text and image query
python test/test_chat.py <video_id> <question> <local_image_path>
```

---

## âš ï¸ Limitations & Future Work

- â±ï¸ **Long Videos** â€” may produce large vector indexes and slower retrieval; segmentation or hierarchical indexing planned.
- ğŸ–¼ï¸ **Frame Sampling** â€” may miss key visuals if extracted too sparsely; scene-change detection under development.
- ğŸ§® **Context Limits** â€” large context may exceed model input size; improved ranking and pruning planned.
- ğŸ”¤ **Additional Modalities** â€” future support for audio embeddings, motion cues, and OCR from frames.
- ğŸ“¡ **Streaming Support** â€” real-time streaming and incremental summarization are in progress.
